%!TEX root = TableSummarizationDemo.tex

\section{Summary of Contributions}\label{sec:summary}

Our system consists of three main components. The first component determines what rules to display to a user based on the user's latest interaction, and the values of parameters such as number of rules to display ($k$), weighting function $W$ to use, and so on. In order to do this, it has to make a pass through the table data several times. This can be expensive for big tables, so we dynamically maintain multiple samples of different parts of the table in memory instead. The second component is responsible for maintaining samples in memory and updating them when required. The third component is a web interface which allows users to explore a dataset using smart drill-down on a web browser. We now describe the first two components in further detail below. The web interface is described in Section~\ref{sec:demo}.

The problem of choosing the optimal rule list of a given size, is NP-Hard. However, we find an approximately optimal solution as follows: We first notice that given a set of rules, a rule-list consisting of those rules has the highest score if the rules are sorted in decreasing order by weight. So we can define the score of a rule {\em set} to be the score of the rule-list obtained by ordering rules of the set in decreasing order by weight. Thus our problem reduces to that of finding the highest scoring rule set. As long as the weight function is monotonic, the score of a rule-set can be shown to be submodular. Then we use the fact that a submodular function can be optimized using a greedy algorithm to choose rules one at a time in a greedy manner until we have $k$ rules. We call the above algorithm BRS (which stands for {\bf B}est {\bf R}ule {\bf S}et). Additional details on our approximation algorithm can be found in our technical report~\cite{tr}. 

The second component of our system is called the `SampleHandler'. To begin with, it takes two user-specified input parameters: the memory capacity $M$, and a parameter called $minSS$. $minSS$ determines the sample size required to run the BRS algorithm. Higher values of $minSS$ increase processing time (since the algorithm has to process a larger amount of data) but also increase the accuracy of the resulting displayed rule-list and rule counts. Then, it maintains a set of samples in memory, such that the sum of sizes of the samples never exceeds $M$. Each sample is a uniformly random subset of tuples that are covered by some rule $r'$. When the user attempts to drill-down on a rule $r$, the SampleHandler appropriately combines tuples from various existing samples to produce a set of $\geq minSS$ tuples covered by $r$. If it can't, it needs to make a pass over the table to generate a new sample. In that case, it determines a new set of samples to create, to maximise the probability that the next user click can be responded using those samples. Then it makes a pass through the table to create the new samples. 
